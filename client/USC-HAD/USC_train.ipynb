{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 ichibanmikan\n",
    "# \n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "# \n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "# \n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class encoder_acc(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN layers applied on acc sensor data to generate pre-softmax\n",
    "    ---\n",
    "    params for __init__():\n",
    "        input_size: e.g. 1\n",
    "        num_classes: e.g. 6\n",
    "    forward():\n",
    "        Input: data\n",
    "        Output: pre-softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Extract features, 2D conv layers\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_size, 64, 2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Conv2d(64, 64, 2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Conv2d(64, 32, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Conv2d(32, 16, 1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 16, -1)#[bsz, 16, 1, 198]\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class encoder_gyr(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN layers applied on acc sensor data to generate pre-softmax\n",
    "    ---\n",
    "    params for __init__():\n",
    "        input_size: e.g. 1\n",
    "        num_classes: e.g. 6\n",
    "    forward():\n",
    "        Input: data\n",
    "        Output: pre-softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Extract features, 2D conv layers\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_size, 64, 2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Conv2d(64, 64, 2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Conv2d(64, 32, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Conv2d(32, 16, 1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 16, -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_acc = encoder_acc(input_size)\n",
    "        self.encoder_gyr = encoder_gyr(input_size)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \n",
    "        acc_output = self.encoder_acc(x1)\n",
    "        gyro_output = self.encoder_gyr(x2)\n",
    "\n",
    "        return acc_output, gyro_output\n",
    "\n",
    "\n",
    "\n",
    "class FMModel(nn.Module):\n",
    "    \"\"\"Model for human-activity-recognition.\"\"\"\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(input_size)\n",
    "\n",
    "        self.gru = nn.GRU(198, 120, 2, batch_first=True)\n",
    "\n",
    "        # Classify output, fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "\n",
    "            nn.Linear(1920, 1280),\n",
    "            nn.BatchNorm1d(1280),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(1280, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(128, num_classes),\n",
    "            )\n",
    "\n",
    "    def forward(self, data_1, data_2):\n",
    "        # last_dim_size = data_1.size(-1)\n",
    "        # split_size = last_dim_size // 2\n",
    "        # data_acc, data_gyr = torch.split(data_1, split_size, dim=-1)\n",
    "\n",
    "        acc_output, gyro_output = self.encoder(data_1, data_2)\n",
    "\n",
    "        fused_feature = (acc_output + gyro_output) / 2 # weighted sum\n",
    "\n",
    "        fused_feature, _ = self.gru(fused_feature)\n",
    "        fused_feature = fused_feature.contiguous().view(fused_feature.size(0), 1920)\n",
    "\n",
    "        output = self.classifier(fused_feature)\n",
    "\n",
    "        return output\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于数据：计划先全部读取出来，然后随机分配\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class rdata:\n",
    "    def __init__(self, data_dir):\n",
    "        data_list_1 = []\n",
    "        data_list_2 = []\n",
    "        labels_list = []\n",
    "        for d in os.listdir(data_dir):\n",
    "            if os.path.isdir(os.path.join(data_dir, d)):\n",
    "                x_tr_1 = np.load(os.path.join(data_dir, d, 'x_train_1.npy'))\n",
    "                x_te_1 = np.load(os.path.join(data_dir, d, 'x_test_1.npy'))\n",
    "                x_tr_2 = np.load(os.path.join(data_dir, d, 'x_train_2.npy'))\n",
    "                x_te_2 = np.load(os.path.join(data_dir, d, 'x_test_2.npy'))\n",
    "                y_tr = np.load(os.path.join(data_dir, d, 'y_train.npy'))\n",
    "                y_te = np.load(os.path.join(data_dir, d, 'y_test.npy'))\n",
    "                for i in range(len(x_tr_1)):\n",
    "                    data_list_1.append(x_tr_1[i])\n",
    "                    labels_list.append(y_tr[i])\n",
    "                    data_list_2.append(x_tr_2[i])\n",
    "                for i in range(len(x_te_1)):\n",
    "                    data_list_1.append(x_te_1[i])\n",
    "                    labels_list.append(y_te[i])\n",
    "                    data_list_2.append(x_te_2[i])\n",
    "        self.data_1 = np.array(data_list_1)\n",
    "        self.data_2 = np.array(data_list_2)\n",
    "        self.data_1 = self.data_1.astype(\"float\")\n",
    "        self.data_2 = self.data_2.astype(\"float\")\n",
    "        self.labels = labels_list  # 150个整数\n",
    "\n",
    "class data_set(Dataset):\n",
    "    def __init__(self, data_1, data_2, data_labels):\n",
    "        super().__init__()\n",
    "        data_1 = data_1.unsqueeze(1) \n",
    "        data_2 = data_2.unsqueeze(1) \n",
    "        self.data_1 = data_1\n",
    "        self.data_2 = data_2\n",
    "        self.labels = data_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_1)\n",
    "    \n",
    "    def __getitem__(self, index):  \n",
    "        return self.data_1[index], self.data_2[index], self.labels[index]\n",
    "    \n",
    "class data_factory:\n",
    "    def __init__(self, data_dir):\n",
    "        self.rdata = rdata(data_dir)\n",
    "    def get_dataset(self):\n",
    "        board_0 = round(len(self.rdata.data_1) * 0.7)\n",
    "        board_1 = round(len(self.rdata.data_1) * 0.7)+round(len(self.rdata.data_1) * 0.15)\n",
    "        \n",
    "        train_data_1 = torch.tensor(self.rdata.data_1[:board_0])\n",
    "        train_data_2 = torch.tensor(self.rdata.data_2[:board_0])\n",
    "        train_labels = torch.tensor(self.rdata.labels[:board_0])\n",
    "        \n",
    "        test_data_1 = torch.tensor(self.rdata.data_1[board_0 : board_1])\n",
    "        test_data_2 = torch.tensor(self.rdata.data_2[board_0 : board_1])\n",
    "        test_labels = torch.tensor(self.rdata.labels[board_0 : board_1])  \n",
    "              \n",
    "        valid_data_1 = torch.tensor(self.rdata.data_1[board_1:])\n",
    "        valid_data_2 = torch.tensor(self.rdata.data_2[board_1:])\n",
    "        valid_labels = torch.tensor(self.rdata.labels[board_1:])  \n",
    "        datasets = [data_set(train_data_1, train_data_2, train_labels), data_set(test_data_1, test_data_2, test_labels), data_set(valid_data_1, valid_data_2, valid_labels)]\n",
    "        dataloaders = [DataLoader(datasets[0], shuffle=True, batch_size=64), DataLoader(datasets[1], shuffle=True, batch_size=64), DataLoader(datasets[2], batch_size=64)]\n",
    "        # return datasets, dataloaders\n",
    "        return dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from __future__ import print_function\n",
    "\n",
    "class TwoCropTransform:\n",
    "    \"\"\"Create two crops of the same image\"\"\"\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return [self.transform(x), self.transform(x)]\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        # print(correct)\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].contiguous().view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "'''作用: 计算模型预测的准确率。支持计算多个 top-k 的准确率，比如 top-1 或 top-5。\n",
    "\n",
    "output 是模型的输出，通常是未经处理的 logits。\n",
    "target 是真实标签。\n",
    "topk 指定需要计算的 k 个准确率，例如 topk=(1, 5) 会计算 top-1 和 top-5 准确率。\n",
    "函数返回一个列表，包含每个 k 对应的准确率（以百分比表示）。'''\n",
    "\n",
    "\n",
    "class train_tools:\n",
    "    def __init__(self, model, config):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.set_optimizer()\n",
    "        \n",
    "    def set_optimizer(self):\n",
    "        self.optimizer = optim.SGD(self.model.parameters(),\n",
    "                        lr=self.config.learning_rate,\n",
    "                        momentum=self.config.momentum,\n",
    "                        weight_decay=self.config.weight_decay)\n",
    "\n",
    "    \n",
    "    def adjust_learning_rate(self, epoch):\n",
    "        lr = self.config.learning_rate\n",
    "        if self.config.cosine:\n",
    "            eta_min = lr * (self.config.lr_decay_rate ** 3)\n",
    "            lr = eta_min + (lr - eta_min) * (\n",
    "                    1 + math.cos(math.pi * epoch / self.config.epochs)) / 2\n",
    "        else:\n",
    "            steps = np.sum(epoch > np.asarray(self.config.lr_decay_epochs))\n",
    "            if steps > 0:\n",
    "                lr = lr * (self.config.lr_decay_rate ** steps)\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "\n",
    "    def warmup_learning_rate(self, epoch, batch_id, total_batches):\n",
    "        if self.config.warm and epoch <= self.config.warm_epochs:\n",
    "            p = (batch_id + (epoch - 1) * total_batches) / \\\n",
    "                (self.config.warm_epochs * total_batches)\n",
    "            lr = self.config.warmup_from + p * (self.config.warmup_to - self.config.warmup_from)\n",
    "\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "\n",
    "    def save_model(self, epoch, save_file):\n",
    "        print('==> Saving...')\n",
    "        state = {\n",
    "            'opt': self.config,\n",
    "            'model': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        torch.save(state, save_file)\n",
    "        del state\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, config, train_loader, valid_loader, device):\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.model = FMModel(1, 13).to(device)\n",
    "        self.model.train()\n",
    "        self.criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "        self.train_tools = train_tools(self.model, config)\n",
    "        self.train_loader = train_loader\n",
    "        self.validater = Validater(self.model, valid_loader, config, self.criterion, device)\n",
    "        self.best_acc = -100\n",
    "    def every_epoch_train(self):\n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "        \n",
    "        end = time.time()\n",
    "        for data_1, data_2, labels in self.train_loader:\n",
    "            data_time.update(time.time() - end)\n",
    "            bsz = data_1.shape[0]\n",
    "            \n",
    "            data_1 = data_1.to(self.device)\n",
    "            data_2 = data_2.to(self.device)\n",
    "\n",
    "            labels = labels.to(self.device)\n",
    "            \n",
    "            output = self.model(data_1, data_2)\n",
    "            loss = self.criterion(output, labels)\n",
    "\n",
    "            acc, _ = accuracy(output, labels, topk=(1, 5))\n",
    "\n",
    "            # update metric\n",
    "            losses.update(loss.item(), bsz)\n",
    "            top1.update(acc[0], bsz)\n",
    "\n",
    "            # SGD\n",
    "            self.train_tools.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.train_tools.optimizer.step()\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "        print(\"loss: %f\", loss.item())\n",
    "        return losses.avg\n",
    "    \n",
    "    def train(self):\n",
    "        record_loss = np.zeros(self.config.epochs)\n",
    "        record_acc = np.zeros(self.config.epochs)\n",
    "        for epoch in range(0, self.config.epochs + 1):\n",
    "            self.train_tools.adjust_learning_rate(epoch)\n",
    "            time1 = time.time()\n",
    "            loss = self.every_epoch_train()\n",
    "            time2 = time.time()\n",
    "            print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n",
    "            record_loss[epoch-1] = loss\n",
    "            # evaluation\n",
    "            loss, val_acc, _ = self.validater.validate()\n",
    "            record_acc[epoch-1] = val_acc\n",
    "            if val_acc > self.best_acc:\n",
    "                self.best_acc = val_acc\n",
    "                # best_confusion = confusion\n",
    "            if self.best_acc > 65.01:\n",
    "                self.train_tools.save_model(epoch, os.path.join(os.getcwd(), 'model/best.pth'))\n",
    "                break;\n",
    "        print(record_acc)\n",
    "class Validater:\n",
    "    def __init__(self, model, valid_loader, config, criterion, device):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.criterion = criterion\n",
    "        self.valid_loader = valid_loader\n",
    "        self.device = device\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        batch_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "\n",
    "        confusion = np.zeros((self.config.num_class, self.config.num_class))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            end = time.time()\n",
    "            for data_1, data_2, labels in self.valid_loader:\n",
    "                \n",
    "                bsz = labels.shape[0]\n",
    "                data_1 = data_1.to(self.device)\n",
    "                data_2 = data_2.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                # forward\n",
    "                output = self.model(data_1, data_2)\n",
    "                loss = self.criterion(output, labels)\n",
    "\n",
    "                # update metric\n",
    "                acc, _ = accuracy(output, labels, topk=(1, 5))\n",
    "                losses.update(loss.item(), bsz)\n",
    "                top1.update(acc[0], bsz)\n",
    "\n",
    "                # calculate and store confusion matrix\n",
    "                # rows = labels.cpu().numpy()\n",
    "                # cols = output.max(1)[1].cpu().numpy()\n",
    "                # for label_index in range(labels.shape[0][0]):\n",
    "                #     confusion[rows[label_index], cols[label_index]] += 1\n",
    "\n",
    "                # measure elapsed time\n",
    "                batch_time.update(time.time() - end)\n",
    "                end = time.time()\n",
    "\n",
    "        return losses.avg, top1.avg, confusion\n",
    "\n",
    "class Tester:\n",
    "    def __init__(self, model, test_loader, device):\n",
    "        self.model = model\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "        \n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        accs = AverageMeter()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data_1, data_2, labels in self.test_loader:\n",
    "                data_1 = data_1.to(self.device)\n",
    "                data_2 = data_2.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                output = self.model(data_1, data_2)\n",
    "                acc, _ = accuracy(output, labels, topk=(1, 5))\n",
    "\n",
    "                # calculate and store confusion matrix\n",
    "                accs.update(acc, data_1.size(0))\n",
    "\n",
    "        return accs.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, config_path) -> None:\n",
    "        self.config_path = config_path\n",
    "        self.load_config()\n",
    "\n",
    "    def load_config(self) -> None:\n",
    "        with open(self.config_path, 'r') as f:\n",
    "            config_data = json.load(f)\n",
    "\n",
    "        self.print_freq = config_data.get('print_freq', 5)\n",
    "        self.save_freq = config_data.get('save_freq', 20)\n",
    "        self.batch_size = config_data.get('batch_size', 16)\n",
    "        self.num_workers = config_data.get('num_workers', 16)\n",
    "        self.epochs = config_data.get('epochs', 99)\n",
    "        self.learning_rate = config_data.get('learning_rate', 0.001)\n",
    "        self.lr_decay_epochs = config_data.get('lr_decay_epochs', '50,100,150')\n",
    "        self.lr_decay_rate = config_data.get('lr_decay_rate', 0.9)\n",
    "        self.weight_decay = config_data.get('weight_decay', 0.0001)\n",
    "        self.momentum = config_data.get('momentum', 0.9)\n",
    "        self.cosine = config_data.get('cosine', True)\n",
    "        self.num_class = config_data.get('num_class', 12)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Config({self.__dict__})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    config = Config(os.path.join(os.getcwd(), 'config.json'))\n",
    "    data_f = data_factory(os.path.join(os.getcwd(), 'datasets/USC-data/'))\n",
    "    train_loader, valid_loader, test_loader = data_f.get_dataset()\n",
    "    tr = Trainer(config, train_loader, valid_loader, device)\n",
    "\n",
    "    tr.train()\n",
    "    print(tr.best_acc)\n",
    "    te = Tester(tr.model, test_loader, device)\n",
    "    acc = te.test()\n",
    "    \n",
    "    print(acc)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
