{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "class gps_encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "        nn.Conv1d(2, 20, 3, padding = 1),\n",
    "        nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "        nn.Conv1d(20, 40, 3, padding = 1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.MaxPool1d(2,padding = 1)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "        nn.Conv1d(40, 80, 3, padding = 1),\n",
    "        nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "        nn.Conv1d(80, 40, 3, padding = 1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.MaxPool1d(2,padding = 1),\n",
    "        nn.Flatten()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "## lidar input: [bsz, 20, 20, 20]\n",
    "class lidar_encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channel = 32\n",
    "        self.dropProb = 0.3\n",
    "\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=20, out_channels=32, kernel_size = (3,3), padding = (1,1)),\n",
    "            nn.ReLU(inplace = True)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size = (3,3), padding = (1,1)),\n",
    "            nn.ReLU(inplace = True),\n",
    "        \n",
    "            nn.Conv2d(64, 128, kernel_size = (3,3), padding = (1,1)),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            nn.Conv2d(128, 32, kernel_size = (3,3), padding = (1,1)),\n",
    "            nn.ReLU(inplace = True)\n",
    "\n",
    "        )\n",
    "\n",
    "        self.maxpool = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=(2, 3)),\n",
    "            nn.Dropout(p = self.dropProb)\n",
    "        )\n",
    "\n",
    "        self.maxpool_ = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=(1, 2)),\n",
    "            nn.Dropout(p = self.dropProb)\n",
    "        )\n",
    "\n",
    "        self.flatten_layer = nn.Sequential(\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = self.layer1(x)\n",
    "        x = a + self.layer2(a)\n",
    "        x = self.maxpool(x) # b\n",
    "\n",
    "        b = x\n",
    "        x = self.layer2(x) + b\n",
    "        x = self.maxpool(x) #c\n",
    "\n",
    "        c = x \n",
    "        x = self.layer2(x) + c \n",
    "        x = self.maxpool_(x) #d\n",
    "\n",
    "        d = x \n",
    "        x = self.layer2(x) + d \n",
    "\n",
    "        x = self.flatten_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "## image input: [bsz, 3, 112, 112]\n",
    "class image_encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channel = 32\n",
    "        self.dropProb = 0.25\n",
    "\n",
    "\n",
    "        self.layer0 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=self.channel, kernel_size = (7,7), padding = (1,1)),\n",
    "            nn.ReLU(inplace = True)\n",
    "        )\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(self.channel, 32, kernel_size = (3,3), padding = (1,1)),\n",
    "            nn.ReLU(inplace = True)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size = (3,3), padding = (1,1)),\n",
    "            nn.ReLU(inplace = True),\n",
    "        \n",
    "            nn.Conv2d(64, 128, kernel_size = (3,3), padding = (1,1)),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            nn.Conv2d(128, 64, kernel_size = (3,3), padding = (1,1)),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            nn.Conv2d(64, 32, kernel_size = (3,3), padding = (1,1)),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "        )\n",
    "\n",
    "        self.maxpool = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=(6, 6)),\n",
    "            nn.Dropout(p = self.dropProb)\n",
    "        )\n",
    "\n",
    "        self.maxpool_ = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=(6, 6)),\n",
    "            nn.Dropout(p = self.dropProb),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        b = x \n",
    "        x = self.layer2(x) + b\n",
    "        x = self.maxpool(x)\n",
    "        c = x \n",
    "        x = self.layer2(x) + c \n",
    "        x = self.maxpool_(x)\n",
    "\n",
    "\n",
    "        return x\n",
    "    \n",
    "              \n",
    "\n",
    "class MySingleModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, modality):\n",
    "        super().__init__()\n",
    "\n",
    "        # print(\"DEBUG: modality is: \", modality)\n",
    "\n",
    "        if modality == 'lidar':\n",
    "            self.encoder = lidar_encoder()\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(160, num_classes),\n",
    "                nn.Softmax()\n",
    "                )\n",
    "        elif modality == 'image':\n",
    "            self.encoder = image_encoder()\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(288, num_classes),\n",
    "                nn.Softmax()\n",
    "                )        \n",
    "        elif modality == 'gps':\n",
    "            self.encoder = gps_encoder()\n",
    "            self.classifier = nn.Sequential(\n",
    "            nn.Linear(40, num_classes),\n",
    "            nn.Softmax()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        feature = self.encoder(x)\n",
    "        output = self.classifier(feature)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class Encoder2_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_1 = gps_encoder()\n",
    "        self.encoder_2 = lidar_encoder()\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "\n",
    "        feature_1 = self.encoder_1(x1)\n",
    "        feature_2 = self.encoder_2(x2)\n",
    "\n",
    "        return feature_1, feature_2\n",
    "\n",
    "class Encoder2_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_1 = gps_encoder()\n",
    "        self.encoder_2 = image_encoder()\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "\n",
    "        feature_1 = self.encoder_1(x1)\n",
    "        feature_2 = self.encoder_2(x2)\n",
    "\n",
    "        return feature_1, feature_2\n",
    "\n",
    "class Encoder2_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_1 = lidar_encoder()\n",
    "        self.encoder_2 = image_encoder()\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "\n",
    "        feature_1 = self.encoder_1(x1)\n",
    "        feature_2 = self.encoder_2(x2)\n",
    "\n",
    "        return feature_1, feature_2\n",
    "\n",
    "\n",
    "class My2Model(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, modality):\n",
    "        super().__init__()\n",
    "\n",
    "        # print(\"DEBUG: modality is: \", modality)\n",
    "\n",
    "        if modality == 'g+l':\n",
    "            self.encoder = Encoder2_1()\n",
    "            self.classifier = nn.Sequential(\n",
    "            nn.Linear(200, num_classes),\n",
    "            nn.Softmax()\n",
    "            )\n",
    "\n",
    "        elif modality == 'g+i':\n",
    "            self.encoder = Encoder2_2()\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(328, num_classes),\n",
    "                nn.Softmax()\n",
    "                ) \n",
    "\n",
    "        elif modality == 'l+i':\n",
    "            self.encoder = Encoder2_3()\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(448, num_classes),\n",
    "                nn.Softmax()\n",
    "                )\n",
    "     \n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # print(x.shape)\n",
    "\n",
    "        feature_1, feature_2 = self.encoder(x1, x2)\n",
    "\n",
    "        feature = torch.cat((feature_1, feature_2), dim=1)\n",
    "        output = self.classifier(feature)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class Encoder3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_1 = gps_encoder()\n",
    "        self.encoder_2 = lidar_encoder()\n",
    "        self.encoder_3 = image_encoder()\n",
    "\n",
    "    def forward(self, x1, x2, x3):  \n",
    "        feature_1 = self.encoder_1(x1)\n",
    "        feature_2 = self.encoder_2(x2)\n",
    "        feature_3 = self.encoder_3(x3)\n",
    "\n",
    "        return feature_1, feature_2, feature_3\n",
    "\n",
    "\n",
    "class My3Model(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder3()\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "        nn.Linear(488, num_classes),\n",
    "        nn.Softmax(dim=1)\n",
    "        )\n",
    "     \n",
    "    def forward(self, x1, x2, x3):\n",
    "        feature_1, feature_2, feature_3 = self.encoder(x1, x2, x3)\n",
    "\n",
    "        feature = torch.cat((feature_1, feature_2, feature_3), dim=1)\n",
    "        output = self.classifier(feature)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import sys\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# 获取当前进程的内存使用情况\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem = process.memory_info()\n",
    "    return mem.rss  # 返回常驻内存集的大小（以字节为单位）\n",
    "# def get_total_size(obj):\n",
    "#     if isinstance(obj, (list, tuple)):\n",
    "#         return sys.getsizeof(obj) + sum(get_total_size(i) for i in obj)\n",
    "#     return sys.getsizeof(obj)\n",
    "\n",
    "class rdata:\n",
    "    def __init__(self, data_dir):\n",
    "        data_list_1 = []\n",
    "        data_list_2 = []\n",
    "        data_list_3 = []\n",
    "        \n",
    "        labels_list = []\n",
    "        print(111, get_memory_usage())\n",
    "        for root, dirs, _ in os.walk(data_dir):\n",
    "            for dir in dirs:\n",
    "                d = os.path.join(root, dir)\n",
    "                if self.count_files_in_directory(d) == 4:\n",
    "                    # print(os.path.join(data_dir, d))\n",
    "                    x_tr_1 = np.load(os.path.join(data_dir, d, 'gps.npz'))\n",
    "                    x_tr_2 = np.load(os.path.join(data_dir, d, 'lidar.npz'))\n",
    "                    x_tr_3 = np.load(os.path.join(data_dir, d, 'image.npz'))\n",
    "                    y_tr = np.load(os.path.join(data_dir, d, 'rf.npz'))\n",
    "                    print(222, get_memory_usage())\n",
    "                    # print(get_memory_usage())\n",
    "                    for i in range(len(x_tr_1['gps'])):\n",
    "                        # print(get_memory_usage())\n",
    "                        # print(len(x_tr_3['image'][i]))\n",
    "                        # print(len(x_tr_3['image'][i][0]))\n",
    "                        # print(len(x_tr_3['image'][i][0][0]))\n",
    "                        \n",
    "                        data_list_1.append(x_tr_1['gps'][i])\n",
    "                        labels_list.append(y_tr['rf'][i])\n",
    "                        data_list_2.append(x_tr_2['lidar'][i])\n",
    "                        data_list_3.append(x_tr_3['image'][i])\n",
    "                print(333, get_memory_usage())\n",
    "            #         l += 1\n",
    "            #     if l > 0:\n",
    "            #         break;\n",
    "            # if l > 0:\n",
    "            #     break;\n",
    "        print(len(data_list_1))\n",
    "        \n",
    "        self.data_1 = np.array(data_list_1)\n",
    "        self.data_2 = np.array(data_list_2)\n",
    "        self.data_3 = np.array(data_list_3)\n",
    "\n",
    "        self.data_1 = self.data_1.astype(\"float\")\n",
    "        self.data_2 = self.data_2.astype(\"float\")\n",
    "        self.data_3 = self.data_3.astype(\"float\")\n",
    "        \n",
    "        self.labels = np.array(labels_list)\n",
    "        \n",
    "    def count_files_in_directory(self, dir_path):\n",
    "        file_count = sum(1 for item in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, item)))\n",
    "        return file_count\n",
    "\n",
    "class data_set(Dataset):\n",
    "    def __init__(self, data_1, data_2, data_3, data_labels):\n",
    "        super().__init__()\n",
    "        self.data_1 = data_1\n",
    "        self.data_2 = data_2\n",
    "        self.data_3 = data_3\n",
    "        self.labels = data_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_1)\n",
    "    \n",
    "    def __getitem__(self, index):  \n",
    "        return self.data_1[index], self.data_2[index], self.data_3[index], self.labels[index]\n",
    "    \n",
    "class data_factory:\n",
    "    def __init__(self, data_dir, config):\n",
    "        self.rdata = rdata(data_dir)\n",
    "        self.config = config\n",
    "    def get_dataset(self):\n",
    "        board_0 = round(len(self.rdata.data_1) * 0.7)\n",
    "        board_1 = round(len(self.rdata.data_1) * 0.7)+round(len(self.rdata.data_1) * 0.15)\n",
    "        \n",
    "        train_data_1 = torch.tensor(self.rdata.data_1[:board_0])\n",
    "        train_data_2 = torch.tensor(self.rdata.data_2[:board_0])\n",
    "        train_data_3 = torch.tensor(self.rdata.data_3[:board_0])  \n",
    "        train_labels = torch.tensor(self.rdata.labels[:board_0])\n",
    "         \n",
    "        test_data_1 = torch.tensor(self.rdata.data_1[board_0 : board_1])\n",
    "        test_data_2 = torch.tensor(self.rdata.data_2[board_0 : board_1])\n",
    "        test_data_3 = torch.tensor(self.rdata.data_3[board_0 : board_1])\n",
    "        \n",
    "        test_labels = torch.tensor(self.rdata.labels[board_0 : board_1])  \n",
    "              \n",
    "        valid_data_1 = torch.tensor(self.rdata.data_1[board_1:])\n",
    "        valid_data_2 = torch.tensor(self.rdata.data_2[board_1:])\n",
    "        valid_data_3 = torch.tensor(self.rdata.data_3[board_1:])\n",
    "        \n",
    "        valid_labels = torch.tensor(self.rdata.labels[board_1:])  \n",
    "        datasets = [data_set(train_data_1, train_data_2, train_data_3, train_labels), data_set(test_data_1, test_data_2, test_data_3, test_labels), data_set(valid_data_1, valid_data_2, valid_data_3, valid_labels)]\n",
    "        dataloaders = [DataLoader(datasets[0], shuffle=True, batch_size=self.config.batch_size), DataLoader(datasets[1], shuffle=True, batch_size=self.config.batch_size), DataLoader(datasets[2], batch_size=self.config.batch_size)]\n",
    "        # return datasets, dataloaders\n",
    "        return dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from __future__ import print_function\n",
    "\n",
    "class TwoCropTransform:\n",
    "    \"\"\"Create two crops of the same image\"\"\"\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return [self.transform(x), self.transform(x)]\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        # print(correct)\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].contiguous().view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "'''作用: 计算模型预测的准确率。支持计算多个 top-k 的准确率，比如 top-1 或 top-5。\n",
    "\n",
    "output 是模型的输出，通常是未经处理的 logits。\n",
    "target 是真实标签。\n",
    "topk 指定需要计算的 k 个准确率，例如 topk=(1, 5) 会计算 top-1 和 top-5 准确率。\n",
    "函数返回一个列表，包含每个 k 对应的准确率（以百分比表示）。'''\n",
    "\n",
    "\n",
    "class train_tools:\n",
    "    def __init__(self, model, config):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.set_optimizer()\n",
    "        \n",
    "    def set_optimizer(self):\n",
    "        self.optimizer = optim.SGD(self.model.parameters(),\n",
    "                        lr=self.config.learning_rate,\n",
    "                        momentum=self.config.momentum,\n",
    "                        weight_decay=self.config.weight_decay)\n",
    "\n",
    "    \n",
    "    def adjust_learning_rate(self, epoch):\n",
    "        lr = self.config.learning_rate\n",
    "        if self.config.cosine:\n",
    "            eta_min = lr * (self.config.lr_decay_rate ** 3)\n",
    "            lr = eta_min + (lr - eta_min) * (\n",
    "                    1 + math.cos(math.pi * epoch / self.config.epochs)) / 2\n",
    "        else:\n",
    "            steps = np.sum(epoch > np.asarray(self.config.lr_decay_epochs))\n",
    "            if steps > 0:\n",
    "                lr = lr * (self.config.lr_decay_rate ** steps)\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "\n",
    "    def warmup_learning_rate(self, epoch, batch_id, total_batches):\n",
    "        if self.config.warm and epoch <= self.config.warm_epochs:\n",
    "            p = (batch_id + (epoch - 1) * total_batches) / \\\n",
    "                (self.config.warm_epochs * total_batches)\n",
    "            lr = self.config.warmup_from + p * (self.config.warmup_to - self.config.warmup_from)\n",
    "\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "\n",
    "    def save_model(self, epoch, save_file):\n",
    "        print('==> Saving...')\n",
    "        state = {\n",
    "            'opt': self.config,\n",
    "            'model': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        torch.save(state, save_file)\n",
    "        del state\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, config, train_loader, valid_loader, device):\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.model = My3Model(self.config.num_classes).to(device)\n",
    "        self.model.train()\n",
    "        self.criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "        self.train_tools = train_tools(self.model, config)\n",
    "        self.train_loader = train_loader\n",
    "        self.validater = Validater(self.model, valid_loader, config, self.criterion, device)\n",
    "        self.best_acc = -100\n",
    "    def every_epoch_train(self):\n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "        \n",
    "        end = time.time()\n",
    "        for data_1, data_2, data_3, labels in self.train_loader:\n",
    "            data_time.update(time.time() - end)\n",
    "            bsz = data_1.shape[0]\n",
    "  \n",
    "            data_1 = data_1.to(self.device)\n",
    "            data_2 = data_2.to(self.device)\n",
    "            data_3 = data_3.to(self.device)\n",
    "\n",
    "            labels = labels.to(self.device)\n",
    "            \n",
    "            output = self.model(data_1, data_2, data_3)\n",
    "            loss = self.criterion(output, labels)\n",
    "\n",
    "            acc, _ = accuracy(output, labels, topk=(1, 5))\n",
    "\n",
    "            # update metric\n",
    "            losses.update(loss.item(), bsz)\n",
    "            top1.update(acc[0], bsz)\n",
    "\n",
    "            # SGD\n",
    "            self.train_tools.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.train_tools.optimizer.step()\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "        print(\"loss: %f\", loss.item())\n",
    "        return losses.avg\n",
    "    \n",
    "    def train(self):\n",
    "        record_loss = np.zeros(self.config.epochs)\n",
    "        record_acc = np.zeros(self.config.epochs)\n",
    "        for epoch in range(0, self.config.epochs + 1):\n",
    "            self.train_tools.adjust_learning_rate(epoch)\n",
    "            time1 = time.time()\n",
    "            loss = self.every_epoch_train()\n",
    "            time2 = time.time()\n",
    "            print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n",
    "            record_loss[epoch-1] = loss\n",
    "            # evaluation\n",
    "            loss, val_acc, _ = self.validater.validate()\n",
    "            record_acc[epoch-1] = val_acc\n",
    "            if val_acc > self.best_acc:\n",
    "                self.best_acc = val_acc\n",
    "                # best_confusion = confusion\n",
    "            if self.best_acc > 65.01:\n",
    "                self.train_tools.save_model(epoch, os.path.join(os.getcwd(), 'model/best.pth'))\n",
    "                break;\n",
    "        print(record_acc)\n",
    "class Validater:\n",
    "    def __init__(self, model, valid_loader, config, criterion, device):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.criterion = criterion\n",
    "        self.valid_loader = valid_loader\n",
    "        self.device = device\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        batch_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "\n",
    "        confusion = np.zeros((self.config.num_classes, self.config.num_classes))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            end = time.time()\n",
    "            for data_1, data_2, data_3, labels in self.valid_loader:\n",
    "                \n",
    "                bsz = labels.shape[0]\n",
    "                data_1 = data_1.to(self.device)\n",
    "                data_2 = data_2.to(self.device)\n",
    "                data_3 = data_3.to(self.device)\n",
    "                \n",
    "                labels = labels.to(self.device)\n",
    "                # forward\n",
    "                output = self.model(data_1, data_2, data_3)\n",
    "                loss = self.criterion(output, labels)\n",
    "\n",
    "                # update metric\n",
    "                acc, _ = accuracy(output, labels, topk=(1, 5))\n",
    "                losses.update(loss.item(), bsz)\n",
    "                top1.update(acc[0], bsz)\n",
    "\n",
    "                # calculate and store confusion matrix\n",
    "                # rows = labels.cpu().numpy()\n",
    "                # cols = output.max(1)[1].cpu().numpy()\n",
    "                # for label_index in range(labels.shape[0][0]):\n",
    "                #     confusion[rows[label_index], cols[label_index]] += 1\n",
    "\n",
    "                # measure elapsed time\n",
    "                batch_time.update(time.time() - end)\n",
    "                end = time.time()\n",
    "\n",
    "        return losses.avg, top1.avg, confusion\n",
    "\n",
    "class Tester:\n",
    "    def __init__(self, model, test_loader, device):\n",
    "        self.model = model\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "        \n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        accs = AverageMeter()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data_1, data_2, data_3, labels in self.test_loader:\n",
    "                data_1 = data_1.to(self.device)\n",
    "                data_2 = data_2.to(self.device)\n",
    "                data_3 = data_3.to(self.device)\n",
    "                \n",
    "                labels = labels.to(self.device)\n",
    "                output = self.model(data_1, data_2, data_3)\n",
    "                acc, _ = accuracy(output, labels, topk=(1, 5))\n",
    "\n",
    "                # calculate and store confusion matrix\n",
    "                accs.update(acc, data_1.size(0))\n",
    "\n",
    "        return accs.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, config_path) -> None:\n",
    "        self.config_path = config_path\n",
    "        self.load_config()\n",
    "\n",
    "    def load_config(self) -> None:\n",
    "        with open(self.config_path, 'r') as f:\n",
    "            config_data = json.load(f)\n",
    "\n",
    "        self.print_freq = config_data.get('print_freq', 5)\n",
    "        self.save_freq = config_data.get('save_freq', 20)\n",
    "        self.batch_size = config_data.get('batch_size', 16)\n",
    "        self.num_workers = config_data.get('num_workers', 16)\n",
    "        self.epochs = config_data.get('epochs', 99)\n",
    "        self.learning_rate = config_data.get('learning_rate', 0.001)\n",
    "        self.lr_decay_epochs = config_data.get('lr_decay_epochs', '50,100,150')\n",
    "        self.lr_decay_rate = config_data.get('lr_decay_rate', 0.9)\n",
    "        self.weight_decay = config_data.get('weight_decay', 0.0001)\n",
    "        self.momentum = config_data.get('momentum', 0.9)\n",
    "        self.cosine = config_data.get('cosine', True)\n",
    "        self.num_classes = config_data.get('num_classes', 12)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Config({self.__dict__})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111 221659136\n",
      "222 225247232\n",
      "111 225247232\n",
      "222 225263616\n",
      "333 1185873920\n",
      "222 1187725312\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2024 ichibanmikan\n",
    "# \n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "# \n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "# \n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(111, get_memory_usage())\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    config = Config(os.path.join(os.getcwd(), 'config.json'))\n",
    "    print(222, get_memory_usage())\n",
    "    data_f = data_factory(os.path.join(os.getcwd(), 'datasets'), config)\n",
    "    train_loader, valid_loader, test_loader = data_f.get_dataset()\n",
    "    tr = Trainer(config, train_loader, valid_loader, device)\n",
    "\n",
    "    tr.train()\n",
    "    print(tr.best_acc)\n",
    "    te = Tester(tr.model, test_loader, device)\n",
    "    acc = te.test()\n",
    "    \n",
    "    print(acc)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
